---
title: "Homework 3"
format: html
editor: visual
---

## Mustafa Mert Türkmen 21902576

# Exercises 3.7

# 4

## 4.a

-   I expect similar results, however; with the cubic regression there will be extra polynomial terms. So, it allows for a closer fit, it means we have more degrees of freedom to our training data. We are increasing the model flexibility linear to cubic, and we work with a small data, hence I expect the cubic model to overfit to any nonlinearities. All these considered, It is strong possibility to have a 'lower' training RSS.

## 4.b

-   I expect the cubic regression will have a higher RSS because it overfits the training data so it would have more error than the linear regression in the test set.

## 4.c

-   In cubic regression we will have higher flexibility, the true relationship between X and Y is not linear so I would expect the cubic regression will have a better fit to non linear data and it is end up with training RSS will be lower than linear regression.

## 4.d

-   There is not enough evidence to tell which RSS would be lower but we can answer this question with knowing the true relationship between X and Y is how far from linear. If it is closer the linear, cubic regression will overfit the training data and linear regression test RSS results will be lower. However, if it is closer to cubic than linear, the cubic regression will have lower test RSS.

# 8

## 8.a

```{r}
library(ISLR2)
```

```{r}
simple_lm <- lm(mpg ~ horsepower, data=Auto)
summary(simple_lm)
```

### 8.a.1

-   After seeing the p-value for horsepower very small (less than 0.05) and the F-statistic is far larger than 1 we can reject the null hypothesis which is there is no relation between them and state that, there is strong evidence for relation between horsepower and mpg.

### 8.a.2

-   t-value is more than 3 and the p value is less than 0 so it is a strong relationship

### 8.a.3

-   It is a negative relationship that can be seen from parameter estimate for horsepower.

### 8.a.4

-   The confidence interval:

```{r}
predict(simple_lm, data.frame(horsepower=95), interval= "confidence", level = 0.95)
```

```{r}
predict(simple_lm, data.frame(horsepower = 98), interval = "prediction", level = 0.95)
```

-   The prediction interval wider than confidence interval.

## 8.b

```{r}
library(ggplot2)
```

```{r}
ggplot(Auto, aes(x = horsepower, y = mpg)) + 
  geom_point() + 
  geom_abline(intercept = coef(simple_lm)[1], slope = coef(simple_lm)[2], col = "red")
```

## 8.c

```{r}
par(mfrow=c(2,2))
plot(simple_lm)
```

-   From part b, I see the nonlinearity. Adding these residuals graph has strengthen this hypothesis.

# Exercises 5.4 - Question 8

## 8.a

```{r}
set.seed(1)
x <- rnorm(100)
y <- x - 2 * x^2 + rnorm(100)
```

-   n = 100, p = 2
-   Y = X - 2X\^2 + ϵ

## 8.b

```{r}
plot(x, y)
```

-   The relation between x and y is not linear. x is between nearly -2 to 2 and y is about -12 to 2.

## 8.c

```{r}
library(boot)
set.seed(22)
df <- data.frame(x, y)
errors <- data.frame()
for (i in 1:4) {
  equations <- glm(y~poly(x,degree=i,raw=TRUE))
  error <- cv.glm(df, equations)$delta[1]
  errors <- rbind(errors, error)
}
colnames(errors) <- c("raw cross-validation estimate of prediction error")
errors
```

## 8.d

```{r}
set.seed(11880)
df <- data.frame(x, y)
errors <- data.frame()
for (i in 1:4) {
  equations <- glm(y~poly(x,degree=i,raw=TRUE))
  error <- cv.glm(df, equations)$delta[1]
  errors <- rbind(errors, error)
}
colnames(errors) <- c("raw cross-validation estimate of prediction error")
errors
```

-   I took same errors since there is no randomness in LOOCV. Leave-one-out cross validation will same for both because it investigates n fold the single observation. It splits the data n-1 train set and one test set. Train a model and evaluates it on the test set. It repeats for n time so test used exactly one for each observation. And takes average of these n errors to obtain the error estimation. \## 8.e

-   I had taken the lowest LOOCV test error rate in the quadratic polynomial. In part b, we plot the x and y and it also looks like a quadratic shape, so it was expected that take lower in second degree.

## 8.f

```{r}
summary(equations)
```

-   p-values show that there is a statistical significance in the model one and two which are linear and quadratic (degree 1 and 2). It also can be said with cross-validation results. There is no evidence that the coefficients for X\^3 and X\^4 are not equal to zero, it can easily seen that looking the p-values (in 95% conf. level).
